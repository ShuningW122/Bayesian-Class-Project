---
title: "HW 8"
author:
  - "Shuning Wang"
  - "April Luo"
  - "Chang Lu" 
date: "`r Sys.Date()`"
echo: false
format:
  gfm:
    toc: true
    html-math-method: webtex
---

```{r}
#| message: false
#| echo: false
library(brms)
library(data.table)
library(here)
library(modelsummary)  # for summarizing data
library(posterior)
library(bayesplot)
library(dplyr)
library(knitr)
library(rmarkdown)
```

# Research Question

> Do effort preferences on rewarded trials in the training section differ between the effort and performance conditions? (model1) 

> Do effort preferences on rewarded trials in the training section differ between the effort and neutral conditions? (model2) 

> Do effort preferences on probe (unrewarded) trials in the training section differ between the effort and performance conditions? (model3) 

> Do effort preferences on probe (unrewarded) trials in the training section differ between the effort and neutral conditons? (model4) 

> Do effort preferences on the dot-motion task in the post-training section differ between the effort and performance conditions? (model5) 

> Do effort preferences on the dot-motion task in the post-training section differ between the effort and neutral conditions? (model6) 

> Do effort preferences on the math task in the post-training section differ between the effort and performance conditions? (model7) 

> Do effort preferences on the math task in the post-training section differ between the effort and neutral conditions? (model8)

#Summary of the study design:

The entire study is divided into three sections: the pre-training section, the training section, and the post-training section. The pre-training section includes two types of cognitive tasks: the dot motion task and the math task. This section is designed to establish a baseline for participants regarding their effort expenditure. The training section is further divided into reward trials and probe trials (unrewarded), with three conditions: reward, performance, and neutral. It employs a between-subjects design to manipulate participants' effort preferences based on the different conditions, but it includes only one cognitive task, which is the dot motion task. In the post-training section, two cognitive tasks are included: the dot motion task and the math task. The dot motion task aims to demonstrate that the manipulation in the training section has a lasting effect over time. Meanwhile, the math task is designed to show that this training not only has a sustained effect on the same task but also exerts an influence across different tasks.

# Variables in dtdotavgwide (dotmotion task dataset)

-   `condition`: performance(The higher the accuracy of the task, the greater the reward), effort(Choosing the hard task yields a high reward, while choosing the easy task results in a low reward), neutral(The reward is a fixed value.).
-   `reward`: The proportion of participants choosing the hard difficulty task in the reward trials during the training section (N_reward = 40 for each subject).
-   `Y_reward`: The number of trials in which participants chose the hard difficulty task during the reward trials in the training section (Y_reward =reward \* N_reward).
-   `probe`: The proportion of participants choosing the hard difficulty task in the unreward trials during the training section (N_probe = 20 for each subject).
-   `Y_probe`: The number of trials in which participants chose the hard difficulty task during the unreward trials in the training section (Y_reward = probe \* N_probe).
-   `pre_training`: The proportion of participants choosing the hard difficulty task in the trials of the pre-training section (N_pretraining = 40).
-   `Y_pretraining`: The number of trials in which participants chose the hard difficulty task during the trials in the pre-training section (Y_pretraining = pre_training \* N_pretraining).
-   `post_training`: The proportion of participants choosing the hard difficulty task in the trials of the post_training section (N_posttraining = 20).
-   `Y_posttraining`: The number of trials in which participants chose the hard difficulty task during the trials in the post-training section (Y_posttraining = post_training \* N_posttraining).

# Variables in dtmathavgwide (math task dataset)

The math task will not appear in the training section (reward trials + probe trials), so the math task dataset does not include any probe or reward trials. - `condition`: Same as above. - `pre_training`: Same as above. - `Y_pretraining`: Same as above. - `post_training`: Same as above. - `Y_posttraining`: Same as above.

## Data Import

```{r}
dtdotavgwide <- fread("/Users/wangshuning/Desktop/USC/24Fall/573Bayesian/project/osfstorage-archive/data/clean/dot_wide.csv")
dtmathavgwide <- fread("/Users/wangshuning/Desktop/USC/24Fall/573Bayesian/project/osfstorage-archive/data/clean/math_wide.csv")

dtdotavgwide[, condition := factor(condition, levels = c("performance", "neutral", "effort"))]
dtmathavgwide[, condition := factor(condition, levels = c("performance", "neutral", "effort"))]

e_vs_p <- c("effort", "performance")
e_vs_n <- c("effort", "neutral")
n_vs_p <- c("neutral", "performance")

```

## Variable Summary

Table @tbl-summ-var1 displays the summary statistics of effort preferences for the dot motion task in the pre-training(baseline) section by condition.

```{r}
#| label: tbl-summ-var1
#| tbl-cap: Descriptive statistics by condition for effort preferences for the dot motion task in the pre-training section
datasummary(Y_pretraining  * 
                (N + Mean + SD + Min + Max + Histogram) ~ 
                factor(condition, labels = c("performance", "neutral", "effort")),
            data = dtdotavgwide)
```

Table @tbl-summ-var2 displays the summary statistics of effort preferences for the dot motion task during the rewarded trials in the training section, categorized by condition.

```{r}
#| label: tbl-summ-var2
#| tbl-cap: Descriptive statistics by condition for effort preferences on rewarded trials in the training section for the dot motion task 
datasummary(Y_reward  * 
                (N + Mean + SD + Min + Max + Histogram) ~ 
                factor(condition, labels = c("performance", "neutral", "effort")),
            data = dtdotavgwide)
```

Table @tbl-summ-var3 displays the summary statistics of effort preferences for the dot motion task during the unrewarded trials (probe trials) in the training section, categorized by condition.

```{r}
#| label: tbl-summ-var3
#| tbl-cap: Descriptive statistics by condition for effort preferences on unrewarded trials (probe trials) in the training section for the dot motion task 
datasummary(Y_probe  * 
                (N + Mean + SD + Min + Max + Histogram) ~ 
                factor(condition, labels = c("performance", "neutral", "effort")),
            data = dtdotavgwide)
```

Table @tbl-summ-var4 displays the summary statistics of effort preferences for the dot motion task in the post-training section by condition.

```{r}
#| label: tbl-summ-var4
#| tbl-cap: Descriptive statistics by condition for effort preferences for the dot motion task in the post-training section
datasummary(Y_posttraining  * 
                (N + Mean + SD + Min + Max + Histogram) ~ 
                factor(condition, labels = c("performance", "neutral", "effort")),
            data = dtdotavgwide)
```

Table @tbl-summ-var5 displays the summary statistics of effort preferences for the math task in the pre-training section by condition.

```{r}
#| label: tbl-summ-var5
#| tbl-cap: Descriptive statistics by condition for effort preferences for the math task in the pre-training section
datasummary(Y_pretraining  * 
                (N + Mean + SD + Min + Max + Histogram) ~ 
                factor(condition, labels = c("performance", "neutral", "effort")),
            data = dtmathavgwide)
```

Table @tbl-summ-var6 displays the summary statistics of effort preferences for the math task in the post-training section by condition.

```{r}
#| label: tbl-summ-var6
#| tbl-cap: Descriptive statistics by condition for effort preferences for the math task in the post-training section
datasummary(Y_posttraining  * 
                (N + Mean + SD + Min + Max + Histogram) ~ 
                factor(condition, labels = c("performance", "neutral", "effort")),
            data = dtmathavgwide)
```

# Model
1)
$$
\begin{aligned}
{Y}_i &\sim \text{Bin}(Nreward_i, \mu_i) \\
logit(\mu_i) &= \eta_i \\
\eta_j &= \beta_0 + \beta_1 (\text{condition}_i)+\beta_2 (\text{pretraining}_i)
\end{aligned}
$$




Prior:
$$
\begin{aligned}
  \beta_0 \sim \mathcal{t4}(0, 2.5) \\
  \quad \beta_1, \beta_2 \sim \mathcal{t4}(0, 1)
\end{aligned}
$$




2) 
$$
\begin{aligned}
{Y}_i &\sim \text{Bin}(Nreward_i, \mu_i) \\
logit(\mu_i) &= \eta_i \\
\eta_j &= \beta_0 + \beta_1 (\text{condition}_i)+\beta_2 (\text{pretraining}_i)
\end{aligned}
$$




Prior:
$$
\begin{aligned}
  \beta_0 \sim \mathcal{t4}(0, 2.5) \\
  \quad \beta_1, \beta_2 \sim \mathcal{t4}(0, 1)
\end{aligned}
$$


3)
$$
\begin{aligned}
{Y}_i &\sim \text{Bin}(Nprobe_i, \mu_i) \\
logit(\mu_i) &= \eta_i \\
\eta_j &= \beta_0 + \beta_1 (\text{condition}_i)+\beta_2 (\text{pretraining}_i)
\end{aligned}
$$




Prior:
$$
\begin{aligned}
  \beta_0 \sim \mathcal{t4}(0, 2.5) \\
  \quad \beta_1, \beta_2 \sim \mathcal{t4}(0, 1)
\end{aligned}
$$


4)
$$
\begin{aligned}
{Y}_i &\sim \text{Bin}(Nprobe_i, \mu_i) \\
logit(\mu_i) &= \eta_i \\
\eta_j &= \beta_0 + \beta_1 (\text{condition}_i)+\beta_2 (\text{pretraining}_i)
\end{aligned}
$$




Prior:
$$
\begin{aligned}
  \beta_0 \sim \mathcal{t4}(0, 2.5) \\
  \quad \beta_1, \beta_2 \sim \mathcal{t4}(0, 1)
\end{aligned}
$$


5)
$$
\begin{aligned}
{Y}_i &\sim \text{Bin}(Npost training_i, \mu_i) \\
logit(\mu_i) &= \eta_i \\
\eta_j &= \beta_0 + \beta_1 (\text{condition}_i)+\beta_2 (\text{pretraining}_i)
\end{aligned}
$$




Prior:
$$
\begin{aligned}
  \beta_0 \sim \mathcal{t4}(0, 2.5) \\
  \quad \beta_1, \beta_2 \sim \mathcal{t4}(0, 1)
\end{aligned}
$$

6)
$$
\begin{aligned}
{Y}_i &\sim \text{Bin}(Nposttraining_i, \mu_i) \\
logit(\mu_i) &= \eta_i \\
\eta_j &= \beta_0 + \beta_1 (\text{condition}_i)+\beta_2 (\text{pretraining}_i)
\end{aligned}
$$




Prior:
$$
\begin{aligned}
  \beta_0 \sim \mathcal{t4}(0, 2.5) \\
  \quad \beta_1, \beta_2 \sim \mathcal{t4}(0, 1)
\end{aligned}
$$

7)
$$
\begin{aligned}
{Y}_i &\sim \text{Bin}(Nposttraining_i, \mu_i) \\
logit(\mu_i) &= \eta_i \\
\eta_j &= \beta_0 + \beta_1 (\text{condition}_i)+\beta_2 (\text{pretraining}_i)
\end{aligned}
$$




Prior:
$$
\begin{aligned}
  \beta_0 \sim \mathcal{t4}(0, 2.5) \\
  \quad \beta_1, \beta_2 \sim \mathcal{t4}(0, 1)
\end{aligned}
$$

8)
$$
\begin{aligned}
{Y}_i &\sim \text{Bin}(Nposttraining_i, \mu_i) \\
logit(\mu_i) &= \eta_i \\
\eta_j &= \beta_0 + \beta_1 (\text{condition}_i)+\beta_2 (\text{pretraining}_i)
\end{aligned}
$$




Prior:
$$
\begin{aligned}
  \beta_0 \sim \mathcal{t4}(0, 2.5) \\
  \quad \beta_1, \beta_2 \sim \mathcal{t4}(0, 1)
\end{aligned}
$$


> Do effort preferences on rewarded trials in the training section differ between the effort and performance conditions? (model1)

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)

model1 <- brm(
  Y_reward|trials(N_reward) ~ condition + Y_pretraining + (1 | subj),  
  data=dtdotavgwide[condition %in% e_vs_p],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 2000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```

# Results

```{r}
summary_results1<-summary(model1)
summary_results1
```

```{r}
plot(model1)
```

```{r}
pp_check(model1)
```

As shown in the rank histogram in @fig-rank-hist-fit1 below, the chains mixed well.

```{r}
#| label: fig-rank-hist-fit1
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws(model1) |>
  mcmc_rank_hist(pars = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept"))
```

@tbl-summ-fit1 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit1
#| tbl-cap: Posterior summary of the model parameters.

summ_fit1 <- as_draws_df(model1) |>
  subset_draws(variable = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept")) |>
  summarise_draws()


knitr::kable(summ_fit1, digits = 2)

```

The analysis showed that effort preferences in rewarded trials under effort conditions were significantly higher than those in the performance condition., with a posterior mean of `r round(summ_fit1$mean[1], 2)` and a 90% CI of [`r round(summ_fit1$q5[1], 2)`, `r round(summ_fit1$q95[1], 2)`].

> Do effort preferences on rewarded trials in the training section differ between the effort and neutral conditions? (model2)

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)

model2 <- brm(
  Y_reward|trials(N_reward) ~ condition + Y_pretraining + (1 | subj),  
  data=dtdotavgwide[condition %in% e_vs_n],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 2000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```

# Results

```{r}
summary_results2<-summary(model2)
summary_results2
```

```{r}
plot(model2)
```

```{r}
pp_check(model2)
```

As shown in the rank histogram in @fig-rank-hist-fit2 below, the chains mixed well.

```{r}
#| label: fig-rank-hist-fit2
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws(model2) |>
  mcmc_rank_hist(pars = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept"))
```

@tbl-summ-fit2 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit2
#| tbl-cap: Posterior summary of the model parameters.

summ_fit2 <- as_draws_df(model2) |>
  subset_draws(variable = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept")) |>
  summarise_draws()


knitr::kable(summ_fit2, digits = 2)

```

The analysis showed that effort preferences in rewarded trials under effort conditions were significantly higher than those in the neutral condition., with a posterior mean of `r round(summ_fit2$mean[1], 2)` and a 90% CI of [`r round(summ_fit2$q5[1], 2)`, `r round(summ_fit2$q95[1], 2)`].




> Do effort preferences on probe (unrewarded) trials in the training section differ between the effort and performance conditions? (model3)

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)

model3 <- brm(
  Y_probe|trials(N_probe) ~ condition + Y_pretraining + (1 | subj),  
  data=dtdotavgwide[condition %in% e_vs_p],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 2000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```

# Results

```{r}
summary_results3<-summary(model3)
summary_results3
```

```{r}
plot(model3)
```

```{r}
pp_check(model3)
```

As shown in the rank histogram in @fig-rank-hist-fit3 below, the chains mixed well.

```{r}
#| label: fig-rank-hist-fit3
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws(model3) |>
  mcmc_rank_hist(pars = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept"))
```

@tbl-summ-fit3 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit3
#| tbl-cap: Posterior summary of the model parameters.

summ_fit3 <- as_draws_df(model3) |>
  subset_draws(variable = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept")) |>
  summarise_draws()


knitr::kable(summ_fit3, digits = 2)

```

The analysis showed that effort preferences in probe trials under effort conditions were not significantly higher than those in the performance condition., with a posterior mean of `r round(summ_fit3$mean[1], 2)` and a 90% CI of [`r round(summ_fit3$q5[1], 2)`, `r round(summ_fit3$q95[1], 2)`].



> Do effort preferences on probe (unrewarded) trials in the training section differ between the effort and neutral conditons? (model4)

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)

model4 <- brm(
  Y_probe|trials(N_probe) ~ condition + Y_pretraining + (1 | subj),  
  data=dtdotavgwide[condition %in% e_vs_n],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 2000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```

# Results

```{r}
summary_results4<-summary(model4)
summary_results4
```

```{r}
plot(model4)
```

```{r}
pp_check(model4)
```

As shown in the rank histogram in @fig-rank-hist-fit4 below, the chains mixed well.

```{r}
#| label: fig-rank-hist-fit4
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws(model4) |>
  mcmc_rank_hist(pars = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept"))
```

@tbl-summ-fit4 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit4
#| tbl-cap: Posterior summary of the model parameters.

summ_fit4 <- as_draws_df(model4) |>
  subset_draws(variable = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept")) |>
  summarise_draws()


knitr::kable(summ_fit4, digits = 3)

```

The analysis showed that effort preferences in probe trials under effort conditions were not significantly higher than those in the neutral condition., with a posterior mean of `r round(summ_fit4$mean[1], 3)` and a 90% CI of [`r round(summ_fit4$q5[1], 3)`, `r round(summ_fit4$q95[1], 3)`].




> Do effort preferences on the dot-motion task in the post-training section differ between the effort and performance conditions? (model5)

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)

model5 <- brm(
  Y_posttraining|trials(N_posttraining) ~ condition + Y_pretraining + (1 | subj),  
  data=dtdotavgwide[condition %in% e_vs_p],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 2000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```

# Results

```{r}
summary_results5<-summary(model5)
summary_results5
```

```{r}
plot(model5)
```

```{r}
pp_check(model5)
```

As shown in the rank histogram in @fig-rank-hist-fit5 below, the chains mixed well.

```{r}
#| label: fig-rank-hist-fit5
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws(model5) |>
  mcmc_rank_hist(pars = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept"))
```

@tbl-summ-fit5 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit5
#| tbl-cap: Posterior summary of the model parameters.

summ_fit5 <- as_draws_df(model5) |>
  subset_draws(variable = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept")) |>
  summarise_draws()


knitr::kable(summ_fit5, digits = 2)

```

The analysis showed that effort preferences in post-training section under effort conditions were  significantly higher than those in the performance condition in dot motion task, with a posterior mean of `r round(summ_fit5$mean[1], 2)` and a 90% CI of [`r round(summ_fit5$q5[1], 2)`, `r round(summ_fit5$q95[1], 2)`].



> Do effort preferences on the dot-motion task in the post-training section differ between the effort and neutral conditions? (model6)

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)

model6 <- brm(
  Y_posttraining|trials(N_posttraining) ~ condition + Y_pretraining + (1 | subj),  
  data=dtdotavgwide[condition %in% e_vs_n],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 2000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```

# Results

```{r}
summary_results6<-summary(model6)
summary_results6
```

```{r}
plot(model6)
```

```{r}
pp_check(model6)
```

As shown in the rank histogram in @fig-rank-hist-fit6 below, the chains mixed well.

```{r}
#| label: fig-rank-hist-fit6
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws(model6) |>
  mcmc_rank_hist(pars = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept"))
```

@tbl-summ-fit6 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit6
#| tbl-cap: Posterior summary of the model parameters.

summ_fit6 <- as_draws_df(model6) |>
  subset_draws(variable = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept")) |>
  summarise_draws()


knitr::kable(summ_fit6, digits = 2)

```


The analysis showed that effort preferences in post-training section under effort conditions were not significantly higher than those in the neutral condition in dot motion task, with a posterior mean of `r round(summ_fit6$mean[1], 2)` and a 90% CI of [`r round(summ_fit6$q5[1], 2)`, `r round(summ_fit6$q95[1], 2)`].


> Do effort preferences on the math task in the post-training section differ between the effort and performance conditions? (model7)

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)

model7 <- brm(
  Y_posttraining|trials(N_posttraining) ~ condition + Y_pretraining + (1 | subj),  
  data=dtmathavgwide[condition %in% e_vs_p],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 2000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```

# Results

```{r}
summary_results7<-summary(model7)
summary_results7
```

```{r}
plot(model7)
```

```{r}
pp_check(model7)
```

As shown in the rank histogram in @fig-rank-hist-fit7 below, the chains mixed well.

```{r}
#| label: fig-rank-hist-fit7
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws(model7) |>
  mcmc_rank_hist(pars = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept"))
```

@tbl-summ-fit7 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit7
#| tbl-cap: Posterior summary of the model parameters.

summ_fit7 <- as_draws_df(model7) |>
  subset_draws(variable = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept")) |>
  summarise_draws()


knitr::kable(summ_fit7, digits = 2)

```

The analysis showed that effort preferences in post-training section under effort conditions were  significantly higher than those in the performance condition in math task, with a posterior mean of `r round(summ_fit7$mean[1], 2)` and a 90% CI of [`r round(summ_fit7$q5[1], 2)`, `r round(summ_fit7$q95[1], 2)`].



> Do effort preferences on the math task in the post-training section differ between the effort and neutral conditions? (model8)

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)

model8 <- brm(
  Y_posttraining|trials(N_posttraining) ~ condition + Y_pretraining + (1 | subj),  
  data=dtmathavgwide[condition %in% e_vs_n],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 2000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```

# Results

```{r}
summary_results8<-summary(model8)
summary_results8
```

```{r}
plot(model8)
```

```{r}
pp_check(model8)
```

As shown in the rank histogram in @fig-rank-hist-fit8 below, the chains mixed well.

```{r}
#| label: fig-rank-hist-fit8
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws(model8) |>
  mcmc_rank_hist(pars = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept"))
```

@tbl-summ-fit8 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit8
#| tbl-cap: Posterior summary of the model parameters.

summ_fit8 <- as_draws_df(model8) |>
  subset_draws(variable = c("b_conditioneffort", "b_Y_pretraining", "b_Intercept")) |>
  summarise_draws()


knitr::kable(summ_fit8, digits = 2)

```

The analysis showed that effort preferences in post-training section under effort conditions were  not significantly higher than those in the neutral condition in math task, with a posterior mean of `r round(summ_fit8$mean[1], 2)` and a 90% CI of [`r round(summ_fit8$q5[1], 2)`, `r round(summ_fit8$q95[1], 2)`].


