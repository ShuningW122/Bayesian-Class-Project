---
title: "Project_accuracy"
author:
  - "Shuning Wang"
  - "April Luo"
  - "Chang Lu" 
date: "`r Sys.Date()`"
echo: false
format:
  gfm:
    toc: true
    html-math-method: webtex
---

```{r}
#| message: false
#| echo: false
library(brms)
library(data.table)
library(here)
library(modelsummary)  # for summarizing data
library(posterior)
library(bayesplot)
library(dplyr)
library(knitr)
library(rmarkdown)
```

## Data Import

```{r}
dot_accuracy <- fread("/Users/wangshuning/Desktop/USC/24Fall/573Bayesian/project/project/project8_accuracy_dotmotion_summary.csv")


dot_accuracy[, condition := factor(condition, levels = c("performance", "neutral", "effort"))]


e_vs_p <- c("effort", "performance")
e_vs_n <- c("effort", "neutral")
n_vs_p <- c("neutral", "performance")

```

> Does accuracy on rewarded trials in the training section differ between the effort and performance conditions?

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)



dot_accuracy <- dot_accuracy %>%
  mutate(
    Training_Reward_trialsNum = ifelse(is.na(Training_Reward_trialsNum) | Training_Reward_trialsNum == 0, 1, Training_Reward_trialsNum),
    Training_Reward_accurate_trialsNum = ifelse(is.na(Training_Reward_accurate_trialsNum), 0, Training_Reward_accurate_trialsNum),
    preTrainingRatio_MeasureError = ifelse(is.na(preTrainingRatio_MeasureError) | preTrainingRatio_MeasureError <= 0, 0.001, preTrainingRatio_MeasureError)
  )



model1 <- brm(
  Training_Reward_accurate_trialsNum|trials(Training_Reward_trialsNum) ~ condition +difficulty+ me(preTraining_accurate_ratio,preTrainingRatio_MeasureError) + (1 | subj),  
  data=dot_accuracy[condition %in% e_vs_p],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 8000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```


# Results

```{r}
summary_results1<-summary(model1)
summary_results1
```

```{r}
plot(model1)
```

```{r}
pp_check(model1)
```

```{r}

#| label: fig-rank-hist-fit
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws_df(model1)|>
  mcmc_rank_hist(pars = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) 

```

@tbl-summ-fit1 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit1
#| tbl-cap: Posterior summary of the model parameters.

summ_fit1 <- as_draws_df(model1) |>
  subset_draws(variable = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) |>
  summarise_draws()


knitr::kable(summ_fit1, digits = 3)

```






