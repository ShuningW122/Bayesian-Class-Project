---
title: "Project_accuracy"
author:
  - "Shuning Wang"
  - "April Luo"
  - "Chang Lu" 
date: "`r Sys.Date()`"
echo: false
format:
  gfm:
    toc: true
    html-math-method: webtex
---

```{r}
#| message: false
#| echo: false
library(brms)
library(data.table)
library(here)
library(modelsummary)  # for summarizing data
library(posterior)
library(bayesplot)
library(dplyr)
library(knitr)
library(rmarkdown)
library(ggplot2)
```

## Data Import

```{r}
dot_accuracy <- fread("/Users/wangshuning/Desktop/USC/24Fall/573Bayesian/project/project/project8_accuracy_dotmotion_summary.csv")
math_accuracy <- fread("/Users/wangshuning/Desktop/USC/24Fall/573Bayesian/project/project/project8_accuracy_math_summary.csv")

dot_accuracy[, condition := factor(condition, levels = c("performance", "neutral", "effort"))]
math_accuracy[, condition := factor(condition, levels = c("performance", "neutral", "effort"))]

e_vs_p <- c("effort", "performance")
e_vs_n <- c("effort", "neutral")
n_vs_p <- c("neutral", "performance")

```


# Research Question

> Does accuracy on rewarded trials in the training section differ between the effort and performance conditions?

> Does accuracy on rewarded trials in the training section differ between the effort and neutral conditions?

> Does accuracy on probe (unrewarded) trials in the training section differ between the effort and performance conditions?

> Does accuracy on probe (unrewarded) trials in the training section differ between the effort and neutral conditons?

> Does accuracy on the dot motion task in the post-training section differ between the effort and performance conditions?

> Does accuracy on the dot motion task in the post-training section differ between the effort and neutral conditions?

> Does accuracy on the math task in the post-training section differ between the effort and performance conditions?

> Does accuracy on the math task in the post-training section differ between the effort and neutral conditions?




## Variable Summary

Table @tbl-summ-var1 displays the summary statistics of accuracy for the dot motion task in the pre-training(baseline) section by condition.

```{r}
#| label: tbl-summ-var1
#| tbl-cap: Descriptive statistics by condition and difficulty for accuracy for the dot motion task in the pre-training section
datasummary(preTraining_accurate_ratio * 
                (N + Mean + SD + Min + Max + Histogram) ~ 
                interaction(factor(condition, labels = c("performance", "neutral", "effort")), 
                            factor(difficulty, labels = c("easy", "hard"))),
            data = dot_accuracy)

```

Table @tbl-summ-var2 displays the summary statistics of accuracy for the dot motion task during the rewarded trials in the training section, categorized by condition.
 
```{r}
#| label: tbl-summ-var2
#| tbl-cap: Descriptive statistics by condition and difficulty for accuracy on rewarded trials in the training section for the dot motion task 
datasummary(Training_Reward_accurate_trialsNum_ratio * 
                (N + Mean + SD + Min + Max + Histogram) ~ 
                interaction(factor(condition, labels = c("performance", "neutral", "effort")), 
                            factor(difficulty, labels = c("easy", "hard"))),
            data = dot_accuracy)
```

Table @tbl-summ-var3 displays the summary statistics of accuracy for the dot motion task during the probe trials in the training section, categorized by condition.
```{r}
#| label: tbl-summ-var3
#| tbl-cap: Descriptive statistics by condition and difficulty for accuracy on rewarded trials in the training section for the dot motion task 
datasummary(Training_probe_accurate_trialsNum_ratio * 
                (N + Mean + SD + Min + Max + Histogram) ~ 
                interaction(factor(condition, labels = c("performance", "neutral", "effort")), 
                            factor(difficulty, labels = c("easy", "hard"))),
            data = dot_accuracy)
```

Table @tbl-summ-var4 displays the summary statistics of accuracy for the dot motion task in the post-training section by condition.

```{r}
#| label: tbl-summ-var4
#| tbl-cap: Descriptive statistics by condition and difficulty for accuracy for the dot motion task in the post-training section
datasummary(postTraining_accurate_ratio * 
                (N + Mean + SD + Min + Max + Histogram) ~ 
                interaction(factor(condition, labels = c("performance", "neutral", "effort")), 
                            factor(difficulty, labels = c("easy", "hard"))),
            data = dot_accuracy)

```

Table @tbl-summ-var5 displays the summary statistics of accuracy for the math task in the pre-training(baseline) section by condition.

```{r}
#| label: tbl-summ-var5
#| tbl-cap: Descriptive statistics by condition and difficulty for accuracy for the math task in the pre-training section
datasummary(preTraining_accurate_ratio * 
                (N + Mean + SD + Min + Max + Histogram) ~ 
                interaction(factor(condition, labels = c("performance", "neutral", "effort")), 
                            factor(difficulty, labels = c("easy", "hard"))),
            data = math_accuracy)

```

Table @tbl-summ-var6 displays the summary statistics of accuracy for the math task in the post-training section by condition.

```{r}
#| label: tbl-summ-var6
#| tbl-cap: Descriptive statistics by condition and difficulty for accuracy for the math task in the post-training section
datasummary(postTraining_accurate_ratio * 
                (N + Mean + SD + Min + Max + Histogram) ~ 
                interaction(factor(condition, labels = c("performance", "neutral", "effort")), 
                            factor(difficulty, labels = c("easy", "hard"))),
            data = math_accuracy)

```

> Does accuracy on rewarded trials in the training section differ between the effort and performance conditions?

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)


dot_accuracy <- dot_accuracy %>%
  mutate(
    Training_Reward_trialsNum = ifelse(is.na(Training_Reward_trialsNum) | Training_Reward_trialsNum == 0, 1, Training_Reward_trialsNum),
    Training_Reward_accurate_trialsNum = ifelse(is.na(Training_Reward_accurate_trialsNum), 0, Training_Reward_accurate_trialsNum),
    preTrainingRatio_MeasureError = ifelse(is.na(preTrainingRatio_MeasureError) | preTrainingRatio_MeasureError <= 0, 0.001, preTrainingRatio_MeasureError)
  )



model1 <- brm(
  Training_Reward_accurate_trialsNum|trials(Training_Reward_trialsNum) ~ condition +difficulty+ me(preTraining_accurate_ratio,preTrainingRatio_MeasureError) + (1 | subj),  
  data=dot_accuracy[condition %in% e_vs_p],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 8000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```


# Results

```{r}
summary_results1<-summary(model1)
summary_results1
```

```{r}
plot(model1)
```

```{r}
pp_check(model1)
```

```{r}
#| label: fig-rank-hist-fit1
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws_df(model1)|>
  mcmc_rank_hist(pars = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) 

```

@tbl-summ-fit1 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit1
#| tbl-cap: Posterior summary of the model parameters.

summ_fit1 <- as_draws_df(model1) |>
  subset_draws(variable = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) |>
  summarise_draws()


knitr::kable(summ_fit1, digits = 3)

```

```{r}
#| label: fig-posterior-desnity1
#| fig-cap: posterior density plot.
draws <- as_draws_df(model1)


params_to_plot <- c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")

mcmc_dens(draws, pars = params_to_plot,color = "lightblue") +
  labs(
    title = "Posterior Density Plots of Model Parameters",
    y = "Posterior Density"  
  ) +
  theme_minimal()

```


> Does accuracy on rewarded trials in the training section differ between the effort and neutral conditions?

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)


dot_accuracy <- dot_accuracy %>%
  mutate(
    Training_Reward_trialsNum = ifelse(is.na(Training_Reward_trialsNum) | Training_Reward_trialsNum == 0, 1, Training_Reward_trialsNum),
    Training_Reward_accurate_trialsNum = ifelse(is.na(Training_Reward_accurate_trialsNum), 0, Training_Reward_accurate_trialsNum),
    preTrainingRatio_MeasureError = ifelse(is.na(preTrainingRatio_MeasureError) | preTrainingRatio_MeasureError <= 0, 0.001, preTrainingRatio_MeasureError)
  )



model2 <- brm(
  Training_Reward_accurate_trialsNum|trials(Training_Reward_trialsNum) ~ condition +difficulty+ me(preTraining_accurate_ratio,preTrainingRatio_MeasureError) + (1 | subj),  
  data=dot_accuracy[condition %in% e_vs_n],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 8000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```


# Results

```{r}
summary_results2<-summary(model2)
summary_results2
```


```{r}
plot(model2)
```


```{r}
pp_check(model2)
```



```{r}
#| label: fig-rank-hist-fit2
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws_df(model2)|>
  mcmc_rank_hist(pars = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) 

```


@tbl-summ-fit2 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit2
#| tbl-cap: Posterior summary of the model parameters.

summ_fit2 <- as_draws_df(model2) |>
  subset_draws(variable = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) |>
  summarise_draws()


knitr::kable(summ_fit2, digits = 3)

```

```{r}
#| label: fig-posterior-desnity2
#| fig-cap: posterior density plot.
draws <- as_draws_df(model2)

params_to_plot <- c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")

mcmc_dens(draws, pars = params_to_plot,color ="blue") +
  labs(
    title = "Posterior Density Plots of Model Parameters",
    y = "Posterior Density"  
  ) +
  theme_minimal()

```



> Does accuracy on probe (unrewarded) trials in the training section differ between the effort and performance conditions?


## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)


dot_accuracy <- dot_accuracy %>%
  mutate(
    Training_probe_trialsNum = ifelse(is.na(Training_probe_trialsNum) | Training_probe_trialsNum == 0, 1, Training_probe_trialsNum),
    Training_probe_accurate_trialsNum = ifelse(is.na(Training_probe_accurate_trialsNum), 0, Training_probe_accurate_trialsNum),
    preTrainingRatio_MeasureError = ifelse(is.na(preTrainingRatio_MeasureError) | preTrainingRatio_MeasureError <= 0, 0.001, preTrainingRatio_MeasureError)
  )



model3 <- brm(
  Training_probe_accurate_trialsNum|trials(Training_probe_trialsNum) ~ condition +difficulty+ me(preTraining_accurate_ratio,preTrainingRatio_MeasureError) + (1 | subj),  
  data=dot_accuracy[condition %in% e_vs_p],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 8000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```

# Results

```{r}
summary_results3<-summary(model3)
summary_results3
```


```{r}
plot(model3)
```


```{r}
pp_check(model3)
```


```{r}
#| label: fig-rank-hist-fit3
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws_df(model3)|>
  mcmc_rank_hist(pars = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) 

```


@tbl-summ-fit3 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit3
#| tbl-cap: Posterior summary of the model parameters.

summ_fit3 <- as_draws_df(model3) |>
  subset_draws(variable = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) |>
  summarise_draws()


knitr::kable(summ_fit3, digits = 3)

```

```{r}
#| label: fig-posterior-desnity3
#| fig-cap: posterior density plot.
draws <- as_draws_df(model3)


params_to_plot <- c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")

mcmc_dens(draws, pars = params_to_plot,color ="pink") +
  labs(
    title = "Posterior Density Plots of Model Parameters",
    y = "Posterior Density"  
  ) +
  theme_minimal()

```



> Does accuracy on probe (unrewarded) trials in the training section differ between the effort and neutral conditons?

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)


dot_accuracy <- dot_accuracy %>%
  mutate(
    Training_probe_trialsNum = ifelse(is.na(Training_probe_trialsNum) | Training_probe_trialsNum == 0, 1, Training_probe_trialsNum),
    Training_probe_accurate_trialsNum = ifelse(is.na(Training_probe_accurate_trialsNum), 0, Training_probe_accurate_trialsNum),
    preTrainingRatio_MeasureError = ifelse(is.na(preTrainingRatio_MeasureError) | preTrainingRatio_MeasureError <= 0, 0.001, preTrainingRatio_MeasureError)
  )



model4 <- brm(
  Training_probe_accurate_trialsNum|trials(Training_probe_trialsNum) ~ condition +difficulty+ me(preTraining_accurate_ratio,preTrainingRatio_MeasureError) + (1 | subj),  
  data=dot_accuracy[condition %in% e_vs_n],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 8000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```

# Results

```{r}
summary_results4<-summary(model4)
summary_results4
```


```{r}
plot(model4)
```


```{r}
pp_check(model4)
```


```{r}
#| label: fig-rank-hist-fit4
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws_df(model4)|>
  mcmc_rank_hist(pars = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) 

```


@tbl-summ-fit4 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit4
#| tbl-cap: Posterior summary of the model parameters.

summ_fit4 <- as_draws_df(model4) |>
  subset_draws(variable = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) |>
  summarise_draws()


knitr::kable(summ_fit4, digits = 3)

```

```{r}
#| label: fig-posterior-desnity4
#| fig-cap: posterior density plot.
draws <- as_draws_df(model4)


params_to_plot <- c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")

mcmc_dens(draws, pars = params_to_plot,color ="purple") +
  labs(
    title = "Posterior Density Plots of Model Parameters",
    y = "Posterior Density"  
  ) +
  theme_minimal()
```



> Does accuracy on the dot motion task in the post-training section differ between the effort and performance conditions?

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)


dot_accuracy <- dot_accuracy %>%
  mutate(
    postTraining_trialsNum = ifelse(is.na(postTraining_trialsNum) | postTraining_trialsNum == 0, 1, postTraining_trialsNum),
    postTraining_accurate_trialsNum = ifelse(is.na(postTraining_accurate_trialsNum), 0, postTraining_accurate_trialsNum),
    preTrainingRatio_MeasureError = ifelse(is.na(preTrainingRatio_MeasureError) | preTrainingRatio_MeasureError <= 0, 0.001, preTrainingRatio_MeasureError)
  )



model5 <- brm(
  postTraining_accurate_trialsNum|trials(postTraining_trialsNum) ~ condition + difficulty+ me(preTraining_accurate_ratio,preTrainingRatio_MeasureError) + (1 | subj),  
  data=dot_accuracy[condition %in% e_vs_p],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 8000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```

# Results

```{r}
summary_results5<-summary(model5)
summary_results5
```


```{r}
plot(model5)
```


```{r}
pp_check(model5)
```


```{r}
#| label: fig-rank-hist-fit5
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws_df(model5)|>
  mcmc_rank_hist(pars = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) 

```


@tbl-summ-fit5 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit5
#| tbl-cap: Posterior summary of the model parameters.

summ_fit5 <- as_draws_df(model5) |>
  subset_draws(variable = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) |>
  summarise_draws()


knitr::kable(summ_fit5, digits = 3)

```

```{r}
#| label: fig-posterior-desnity5
#| fig-cap: posterior density plot.
draws <- as_draws_df(model5)

params_to_plot <- c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")

mcmc_dens(draws, pars = params_to_plot,color ="lightgreen") +
  labs(
    title = "Posterior Density Plots of Model Parameters",
    y = "Posterior Density"  
  ) +
  theme_minimal()
```



> Does accuracy on the dot motion task in the post-training section differ between the effort and neutral conditions?

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)


dot_accuracy <- dot_accuracy %>%
  mutate(
    postTraining_trialsNum = ifelse(is.na(postTraining_trialsNum) | postTraining_trialsNum == 0, 1, postTraining_trialsNum),
    postTraining_accurate_trialsNum = ifelse(is.na(postTraining_accurate_trialsNum), 0, postTraining_accurate_trialsNum),
    preTrainingRatio_MeasureError = ifelse(is.na(preTrainingRatio_MeasureError) | preTrainingRatio_MeasureError <= 0, 0.001, preTrainingRatio_MeasureError)
  )



model6 <- brm(
  postTraining_accurate_trialsNum|trials(postTraining_trialsNum) ~ condition + difficulty+ me(preTraining_accurate_ratio,preTrainingRatio_MeasureError) + (1 | subj),  
  data=dot_accuracy[condition %in% e_vs_n],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 8000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```

# Results

```{r}
summary_results6<-summary(model6)
summary_results6
```


```{r}
plot(model6)
```


```{r}
pp_check(model6)
```


```{r}
#| label: fig-rank-hist-fit6
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws_df(model6)|>
  mcmc_rank_hist(pars = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) 

```


@tbl-summ-fit6 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit6
#| tbl-cap: Posterior summary of the model parameters.

summ_fit6 <- as_draws_df(model6) |>
  subset_draws(variable = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) |>
  summarise_draws()


knitr::kable(summ_fit6, digits = 3)

```

```{r}
#| label: fig-posterior-desnity6
#| fig-cap: posterior density plot.
draws <- as_draws_df(model6)


params_to_plot <- c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")

mcmc_dens(draws, pars = params_to_plot,color ="darkblue") +
  labs(
    title = "Posterior Density Plots of Model Parameters",
    y = "Posterior Density"  
  ) +
  theme_minimal()
```



> Does accuracy on the math task in the post-training section differ between the effort and performance conditions?

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)


math_accuracy <- math_accuracy %>%
  mutate(
    postTraining_trialsNum = ifelse(is.na(postTraining_trialsNum) | postTraining_trialsNum == 0, 1, postTraining_trialsNum),
    postTraining_accurate_trialsNum = ifelse(is.na(postTraining_accurate_trialsNum), 0, postTraining_accurate_trialsNum),
    preTrainingRatio_MeasureError = ifelse(is.na(preTrainingRatio_MeasureError) | preTrainingRatio_MeasureError <= 0, 0.001, preTrainingRatio_MeasureError)
  )



model7 <- brm(
  postTraining_accurate_trialsNum|trials(postTraining_trialsNum) ~ condition + difficulty+ me(preTraining_accurate_ratio,preTrainingRatio_MeasureError) + (1 | subj),  
  data=math_accuracy[condition %in% e_vs_p],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 8000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```

# Results

```{r}
summary_results7<-summary(model7)
summary_results7
```


```{r}
plot(model7)
```


```{r}
pp_check(model7)
```



```{r}
#| label: fig-rank-hist-fit7
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws_df(model7)|>
  mcmc_rank_hist(pars = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) 

```


@tbl-summ-fit7 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit7
#| tbl-cap: Posterior summary of the model parameters.

summ_fit7 <- as_draws_df(model7) |>
  subset_draws(variable = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) |>
  summarise_draws()


knitr::kable(summ_fit7, digits = 3)

```

```{r}
#| label: fig-posterior-desnity7
#| fig-cap: posterior density plot.
draws <- as_draws_df(model7)

params_to_plot <- c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")

mcmc_dens(draws, pars = params_to_plot,color ="lightgray") +
  labs(
    title = "Posterior Density Plots of Model Parameters",
    y = "Posterior Density"  
  ) +
  theme_minimal()
```



> Does accuracy on the math task in the post-training section differ between the effort and neutral conditions?

## Analysis

We used 4 chains, each with 8,000 iterations (first 4,000 as warm-ups).

```{r}
#| include: false
priors <- c(
  prior(student_t(4, 0, 1), class = "b"),           
  prior(student_t(4, 0, 2.5), class = "Intercept")  
  
)


math_accuracy <- math_accuracy %>%
  mutate(
    postTraining_trialsNum = ifelse(is.na(postTraining_trialsNum) | postTraining_trialsNum == 0, 1, postTraining_trialsNum),
    postTraining_accurate_trialsNum = ifelse(is.na(postTraining_accurate_trialsNum), 0, postTraining_accurate_trialsNum),
    preTrainingRatio_MeasureError = ifelse(is.na(preTrainingRatio_MeasureError) | preTrainingRatio_MeasureError <= 0, 0.001, preTrainingRatio_MeasureError)
  )



model8 <- brm(
  postTraining_accurate_trialsNum|trials(postTraining_trialsNum) ~ condition + difficulty+ me(preTraining_accurate_ratio,preTrainingRatio_MeasureError) + (1 | subj),  
  data=math_accuracy[condition %in% e_vs_n],                                    
  family = binomial(link = "logit"),                      
  prior = priors,
  iter = 8000,                                            
  chains = 4, 
  control = list(adapt_delta = 0.99),
  cores = 4,
  save_pars = save_pars(all = TRUE)
)

```

# Results

```{r}
summary_results8<-summary(model8)
summary_results8
```


```{r}
plot(model8)
```


```{r}
pp_check(model8)
```



```{r}
#| label: fig-rank-hist-fit8
#| fig-cap: Rank histogram of the posterior distributions of model parameters.
as_draws_df(model8)|>
  mcmc_rank_hist(pars = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) 

```


@tbl-summ-fit8 shows the posterior distributions of b_conditioneffort, b_Y_pretraining, b_Intercept.

```{r}
#| label: tbl-summ-fit8
#| tbl-cap: Posterior summary of the model parameters.

summ_fit8 <- as_draws_df(model8) |>
  subset_draws(variable = c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")) |>
  summarise_draws()


knitr::kable(summ_fit8, digits = 3)

```

```{r}
#| label: fig-posterior-desnity8
#| fig-cap: posterior density plot.
draws <- as_draws_df(model8)


params_to_plot <- c("b_Intercept", "b_conditioneffort", "b_difficultyhard", "bsp_mepreTraining_accurate_ratiopreTrainingRatio_MeasureError")

mcmc_dens(draws, pars = params_to_plot,color ="darkgrey") +
  labs(
    title = "Posterior Density Plots of Model Parameters",
    y = "Posterior Density"  
  ) +
  theme_minimal()
```


